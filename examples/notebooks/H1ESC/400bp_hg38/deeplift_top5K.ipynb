{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import deeplift\n",
    "import evautils\n",
    "from evautils import sequtils\n",
    "from evautils import kerasutils\n",
    "from evautils import dirutils\n",
    "from evautils import impscoringutils\n",
    "from __future__ import print_function\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "REGION_SIZE = 400\n",
    "CELL_LINE = 'H1ESC'\n",
    "POS_PREFIX = CELL_LINE +'_' + str(REGION_SIZE)\n",
    "MASTER_DIR='/users/eprakash/benchmarking/H1ESC/400bp_hg38'\n",
    "DL_BASE_DIR=MASTER_DIR+'/deeplift'\n",
    "PREPROCESSING_BASE_DIR = MASTER_DIR+'/preprocessing'\n",
    "TRAINING_BASE_DIR=MASTER_DIR+'/training'\n",
    "MOMMA_DRAGONN=TRAINING_BASE_DIR+'/'+'momma_dragonn'\n",
    "IMPLANTED_POS_BED_FILE = PREPROCESSING_BASE_DIR + '/' + 'implanted_' + POS_PREFIX + '.bed.gz'\n",
    "MODEL_PREFIX='record_1_model_XHjBt_'\n",
    "MODEL=MOMMA_DRAGONN+'/examples/fasta_sequential_model/model_files/'+MODEL_PREFIX+'modelJson.json'\n",
    "WEIGHTS=MOMMA_DRAGONN+'/examples/fasta_sequential_model/model_files/'+MODEL_PREFIX+'modelWeights.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory /users/eprakash/benchmarking/H1ESC/400bp_hg38/deeplift already exists\n"
     ]
    }
   ],
   "source": [
    "dirutils.createDir(DL_BASE_DIR, mustcreate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Loading /users/eprakash/benchmarking/H1ESC/400bp_hg38/preprocessing/implanted_H1ESC_400.bed.gz ...\n",
      "#Loaded 96663 sequences from /users/eprakash/benchmarking/H1ESC/400bp_hg38/preprocessing/implanted_H1ESC_400.bed.gz\n",
      "Got 96663 positive sequences\n",
      "Sequences length:  96663\n"
     ]
    }
   ],
   "source": [
    "data_filename_positive = IMPLANTED_POS_BED_FILE\n",
    "labeled_sequences = sequtils.load_sequences_from_bedfile(data_filename_positive)\n",
    "print(\"Got %d positive sequences\" % len(labeled_sequences))\n",
    "positive_labels = labeled_sequences.keys()\n",
    "labels = labeled_sequences.keys()\n",
    "sequences =labeled_sequences.values()\n",
    "print(\"Sequences length: \", len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sequtils.removeUnsupportedChars(sequences, labels, labeled_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "onehot_data = np.array([sequtils.one_hot_encode_along_channel_axis(seq) for seq in sequences])\n",
    "print(onehot_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keras_model=kerasutils.load_keras_model_using_json(MODEL, WEIGHTS)\n",
    "keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = keras_model.predict(onehot_data)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_5k_pos_labels, top_pos_seqs=kerasutils.getTopNumPos(labels, labeled_sequences, preds, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h5f = h5py.File(DL_BASE_DIR+'/'+POS_PREFIX+'_top_5K_pos_labels.h5', 'w')\n",
    "h5f.create_dataset('labels', data=top_5k_pos_labels)\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Restrict onehot_data to the top pos seqs\n",
    "onehot_data = np.array([sequtils.one_hot_encode_along_channel_axis(seq) for seq in top_pos_seqs])\n",
    "print(onehot_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "method_to_model=kerasutils.prepareDLModel(WEIGHTS, MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make sure predictions are the same as the original model\n",
    "from deeplift.util import compile_func\n",
    "model_to_test = method_to_model['rescale_conv_revealcancel_fc']\n",
    "kerasutils.sanityCheck(model_to_test, onehot_data, keras_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "method_to_scoring_func = impscoringutils.compileScoringFunctions(method_to_model)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "method_to_task_to_scores = OrderedDict()\n",
    "all_zeroes_methods=['grad_times_inp', 'rescale_all_layers']\n",
    "avg_gc_methods=['rescale_all_layers']\n",
    "multiref_methods=['rescale_all_layers', 'rescale_conv_revealcancel_fc']\n",
    "ig=['integrated_gradients10']\n",
    "h5f = h5py.File(DL_BASE_DIR+'/'+POS_PREFIX+'_dl_scores_5K.h5', 'w')\n",
    "h5f.create_dataset(\"labels\", data=top_5k_pos_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "impscoringutils.flatRefScore(method_to_task_to_scores, method_to_scoring_func, ig, onehot_data, 0)\n",
    "print(\"Done!\")\n",
    "for meth in method_to_task_to_scores.keys():\n",
    "    print(\"Storing scores for \" + str(meth))\n",
    "    h5f.create_dataset(meth, data=method_to_task_to_scores[meth][0])\n",
    "method_to_task_to_scores.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "impscoringutils.flatRefScore(method_to_task_to_scores, method_to_scoring_func, all_zeroes_methods, onehot_data, 0)\n",
    "print(\"Done!\")\n",
    "for meth in method_to_task_to_scores.keys():\n",
    "    print(\"Storing scores for \" + str(meth))\n",
    "    h5f.create_dataset(meth, data=method_to_task_to_scores[meth][0])\n",
    "method_to_task_to_scores.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "impscoringutils.flatRefScore(method_to_task_to_scores, method_to_scoring_func, ig, onehot_data, 1)\n",
    "print(\"Done!\")\n",
    "for meth in method_to_task_to_scores.keys():\n",
    "    print(\"Storing scores for \" + str(meth))\n",
    "    h5f.create_dataset(meth, data=method_to_task_to_scores[meth][0])\n",
    "method_to_task_to_scores.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "impscoringutils.flatRefScore(method_to_task_to_scores, method_to_scoring_func, avg_gc_methods, onehot_data, 1)\n",
    "print(\"Done!\")\n",
    "for meth in method_to_task_to_scores.keys():\n",
    "    print(\"Storing scores for \" + str(meth))\n",
    "    h5f.create_dataset(meth, data=method_to_task_to_scores[meth][0])\n",
    "method_to_task_to_scores.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "impscoringutils.multirefScore(method_to_task_to_scores, method_to_scoring_func, ig, top_pos_seqs)\n",
    "print(\"Done!\")\n",
    "for meth in method_to_task_to_scores.keys():\n",
    "    print(\"Storing scores for \" + str(meth))\n",
    "    h5f.create_dataset(meth, data=method_to_task_to_scores[meth][0])\n",
    "method_to_task_to_scores.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "impscoringutils.multirefScore(method_to_task_to_scores, method_to_scoring_func, multiref_methods, top_pos_seqs)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for meth in method_to_task_to_scores.keys():\n",
    "    print(\"Storing scores for \" + str(meth))\n",
    "    h5f.create_dataset(meth, data=method_to_task_to_scores[meth][0])\n",
    "h5f.close()\n",
    "method_to_task_to_scores.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
